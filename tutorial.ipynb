{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare some data\n",
    "bunch = load_boston()\n",
    "y_train = bunch.target[0:250]\n",
    "y_test = bunch.target[250:506]\n",
    "X_train = pd.DataFrame(bunch.data[0:250], columns=bunch.feature_names)\n",
    "X_test = pd.DataFrame(bunch.data[250:506], columns=bunch.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import erf, erfinv\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "class GuassRankScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Standardize features by removing the mean and scaling to unit variance\n",
    "        The standard score of a sample `x` is calculated as:\n",
    "            z = (x - u) / s\n",
    "        where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
    "        and `s` is the standard deviation of the training samples or one if\n",
    "        `with_std=False`.\n",
    "        Centering and scaling happen independently on each feature by computing\n",
    "        the relevant statistics on the samples in the training set. Mean and\n",
    "        standard deviation are then stored to be used on later data using the\n",
    "        `transform` method.\n",
    "        Standardization of a dataset is a common requirement for many\n",
    "        machine learning estimators: they might behave badly if the\n",
    "        individual features do not more or less look like standard normally\n",
    "        distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "        For instance many elements used in the objective function of\n",
    "        a learning algorithm (such as the RBF kernel of Support Vector\n",
    "        Machines or the L1 and L2 regularizers of linear models) assume that\n",
    "        all features are centered around 0 and have variance in the same\n",
    "        order. If a feature has a variance that is orders of magnitude larger\n",
    "        that others, it might dominate the objective function and make the\n",
    "        estimator unable to learn from other features correctly as expected.\n",
    "        This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
    "        `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
    "        Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        copy : boolean, optional, default True\n",
    "            If False, try to avoid a copy and do inplace scaling instead.\n",
    "            This is not guaranteed to always work inplace; e.g. if the data is\n",
    "            not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
    "            returned.\n",
    "        with_mean : boolean, True by default\n",
    "            If True, center the data before scaling.\n",
    "            This does not work (and will raise an exception) when attempted on\n",
    "            sparse matrices, because centering them entails building a dense\n",
    "            matrix which in common use cases is likely to be too large to fit in\n",
    "            memory.\n",
    "        with_std : boolean, True by default\n",
    "            If True, scale the data to unit variance (or equivalently,\n",
    "            unit standard deviation).\n",
    "        Attributes\n",
    "        ----------\n",
    "        scale_ : ndarray or None, shape (n_features,)\n",
    "            Per feature relative scaling of the data. This is calculated using\n",
    "            `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n",
    "            .. versionadded:: 0.17\n",
    "               *scale_*\n",
    "        mean_ : ndarray or None, shape (n_features,)\n",
    "            The mean value for each feature in the training set.\n",
    "            Equal to ``None`` when ``with_mean=False``.\n",
    "        var_ : ndarray or None, shape (n_features,)\n",
    "            The variance for each feature in the training set. Used to compute\n",
    "            `scale_`. Equal to ``None`` when ``with_std=False``.\n",
    "        n_samples_seen_ : int or array, shape (n_features,)\n",
    "            The number of samples processed by the estimator for each feature.\n",
    "            If there are not missing samples, the ``n_samples_seen`` will be an\n",
    "            integer, otherwise it will be an array.\n",
    "            Will be reset on new calls to fit, but increments across\n",
    "            ``partial_fit`` calls.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.copy = copy\n",
    "        self.interp_params = {'kind': interp_kind, 'copy': interp_copy, 'fill_value': 'extrapolate'}\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the mean and std to be used for later scaling.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to compute the mean and standard deviation\n",
    "            used for later scaling along the features axis.\n",
    "        y\n",
    "            Ignored\n",
    "        \"\"\"\n",
    "        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        ranks = np.argsort(np.argsort(X, axis=0), axis=0)\n",
    "        bound = 1.0 - self.epsilon\n",
    "        factors = np.max(ranks) / 2.0 * bound\n",
    "        scaled_ranks = np.clip(ranks / factors - bound, -bound, bound)\n",
    "\n",
    "        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(interp1d)(x, r, **self.interp_params) for x, r in zip(X.T, scaled_ranks.T))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"Perform standardization by centering and scaling\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _transform(self, i, x):\n",
    "        return erfinv(self.interp_func_[i](x))\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        \"\"\"Scale back the data to the original representation\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _inverse_transform(self, i, x):\n",
    "        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, **self.interp_params)\n",
    "        return inv_interp_func(erf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_rank_scaler = GuassRankScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      194\n",
       "1      120\n",
       "2      121\n",
       "3      122\n",
       "4      123\n",
       "5      124\n",
       "6      187\n",
       "7      186\n",
       "8      192\n",
       "9      185\n",
       "10     184\n",
       "11     183\n",
       "12     191\n",
       "13     125\n",
       "14     126\n",
       "15     127\n",
       "16     128\n",
       "17     129\n",
       "18     130\n",
       "19     131\n",
       "20     132\n",
       "21     133\n",
       "22     134\n",
       "23     135\n",
       "24     136\n",
       "25     137\n",
       "26     138\n",
       "27     119\n",
       "28     118\n",
       "29     117\n",
       "      ... \n",
       "220    181\n",
       "221    182\n",
       "222    176\n",
       "223    179\n",
       "224    174\n",
       "225    173\n",
       "226    172\n",
       "227    175\n",
       "228    170\n",
       "229    169\n",
       "230    168\n",
       "231    167\n",
       "232    166\n",
       "233    142\n",
       "234    144\n",
       "235    140\n",
       "236    116\n",
       "237    107\n",
       "238    218\n",
       "239    219\n",
       "240    220\n",
       "241    221\n",
       "242    223\n",
       "243    222\n",
       "244    203\n",
       "245    202\n",
       "246    201\n",
       "247    200\n",
       "248    204\n",
       "249    199\n",
       "Name: ZN, Length: 250, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(np.argsort(X_train['ZN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sklearn_preprocessing/lib/python3.7/site-packages/scipy/interpolate/interpolate.py:609: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
      "/anaconda3/envs/sklearn_preprocessing/lib/python3.7/site-packages/scipy/interpolate/interpolate.py:612: RuntimeWarning: invalid value encountered in multiply\n",
      "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n"
     ]
    }
   ],
   "source": [
    "a = gauss_rank_scaler.fit_transform(X_train[['CRIM', 'ZN', 'RM', 'AGE', 'DIS', 'B', 'LSTAT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54422033,        nan,        nan,        nan,        nan,\n",
       "              nan, 0.44420389, 0.44420389, 0.44420389, 0.44420389,\n",
       "       0.44420389, 0.44420389, 0.44420389,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan, 1.05349342,\n",
       "       1.05349342,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "       0.55384382, 0.55384382, 0.55384382, 0.55384382, 1.05349342,\n",
       "       1.45320962, 1.39885243, 2.75106391, 0.65654768, 0.65654768,\n",
       "       0.65654768, 0.65654768, 0.65654768, 0.65654768, 0.53469661,\n",
       "       1.1232776 , 1.1232776 , 0.44420389, 0.44420389, 0.44420389,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "       0.65654768, 0.65654768, 0.65654768, 0.65654768,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan, 0.77540784, 0.77540784, 0.77540784,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan, 0.90514063, 0.90514063, 0.90514063,\n",
       "       0.90514063, 0.90514063, 0.90514063, 1.01212777, 1.01212777,\n",
       "       1.1232776 , 1.1232776 , 1.1232776 , 1.1232776 , 1.51781714,\n",
       "       1.51781714, 1.3098691 , 1.3098691 , 1.51781714, 1.51781714,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan, 0.8156207 , 0.8156207 ,\n",
       "       0.8156207 , 0.8156207 , 0.8156207 , 0.8156207 , 0.59341727,\n",
       "       0.59341727, 0.59341727, 0.59341727, 0.59341727, 0.59341727])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort([1.0, 3.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfinv\n",
    "\n",
    "class GaussRankScaler():\n",
    "\n",
    "\tdef __init__( self ):\n",
    "\t\tself.epsilon = 0.001\n",
    "\t\tself.lower = -1 + self.epsilon\n",
    "\t\tself.upper =  1 - self.epsilon\n",
    "\t\tself.range = self.upper - self.lower\n",
    "\n",
    "\tdef fit_transform( self, X ):\n",
    "\t\n",
    "\t\ti = np.argsort( X, axis = 0 )\n",
    "\t\tj = np.argsort( i, axis = 0 )\n",
    "\n",
    "\t\tassert ( j.min() == 0 ).all()\n",
    "\t\tassert ( j.max() == len( j ) - 1 ).all()\n",
    "\t\t\n",
    "\t\tj_range = len( j ) - 1\n",
    "\t\tself.divider = j_range / self.range\n",
    "\t\t\n",
    "\t\ttransformed = j / self.divider\n",
    "\t\ttransformed = transformed - self.upper\n",
    "\t\ttransformed = erfinv( transformed )\n",
    "\t\t\n",
    "\t\treturn transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rankgauss_trafo(x):\n",
    "    finite_indices = np.isfinite(x)\n",
    "    if np.sum(finite_indices) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    x_finite = x[np.isfinite(x)]\n",
    " \n",
    "    hist = dict()\n",
    "    for val in x_finite:\n",
    "        hist[val] = hist.get(val, 0) + 1\n",
    " \n",
    "    len_hist = len(hist)\n",
    "    list_keys = list(hist.keys())\n",
    " \n",
    "    if len_hist == 1:\n",
    "        return np.array(list_keys), np.array([0.0])\n",
    "    elif len_hist == 2:\n",
    "        return np.array(list_keys), np.array([0.0, 1.0])\n",
    "    else:\n",
    "        hist = OrderedDict(sorted(hist.items()))    # sort by key\n",
    "        n = float(x_finite.shape[0])\n",
    "        cnt = 0.0\n",
    "        mean = 0.0\n",
    "        trafo_keys = list()\n",
    "        trafo_values = list()\n",
    " \n",
    "        for key, val in hist.items():\n",
    "            # (notice) 'cnt / n * 0.998 + 1e-3' is always larger than zero\n",
    "            rank_v = norm_cdf_inv(cnt / n * 0.998 + 1e-3) * 0.7\n",
    "            trafo_keys.append(key)\n",
    "            trafo_values.append(rank_v)\n",
    "            mean += val * rank_v\n",
    "            cnt += val\n",
    " \n",
    "        mean /= n\n",
    "        trafo_values = np.array(trafo_values)\n",
    "        trafo_values -= mean\n",
    " \n",
    "        return np.array(trafo_keys), trafo_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import erf, erfinv\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "class GuassRankScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, epsilon=1e-4, kind='linear', copy=False, n_jobs=None):\n",
    "        self.epsilon = epsilon\n",
    "        self.interp_params = {'kind': kind, 'copy': copy, 'fill_value': 'extrapolate'}\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        ranks = np.argsort(np.argsort(X, axis=0), axis=0)\n",
    "        bound = 1.0 - self.epsilon\n",
    "        factors = np.max(ranks) / 2.0 * bound\n",
    "        scaled_ranks = np.clip(ranks / factors - bound, -bound, bound)\n",
    "\n",
    "        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(interp1d)(x, r, **self.interp_params) for x, r in zip(X.T, scaled_ranks.T))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _transform(self, i, x):\n",
    "        return erfinv(self.interp_func_[i](x))\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "        X = check_array(X, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _inverse_transform(self, i, x):\n",
    "        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, **self.interp_params)\n",
    "        return inv_interp_func(erf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GuassRankScaler(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_train[['CRIM', 'RM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v = check_array(X_tr, dtype=FLOAT_DTYPES, force_all_finite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.argsort(np.argsort(X_v, axis=0), axis=0)\n",
    "bound = 1.0 - 0.0001\n",
    "factors = np.max(ranks) / 2.0 * bound\n",
    "scaled_ranks = np.clip(ranks / factors - bound, -bound, bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for x, r in zip(X_v.T, scaled_ranks.T):\n",
    "    a.append(interp1d(x, r, **interp_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_params = {'kind': 'linear', 'copy': False, 'fill_value': 'extrapolate'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_func_ = Parallel(n_jobs=None)(\n",
    "    delayed(interp1d)(x, r, **interp_params) for x, r in (X_v.T, scaled_ranks.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform(i, x):\n",
    "    return interp_func_[i](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_func_[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs=None)(delayed(_transform)(i, x) for i, x in enumerate(X_v.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.interp_func_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.argsort(np.argsort(X_tr, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bound = 1.0 - 0.001\n",
    "factor = np.max(ranks) / 2.0 * bound\n",
    "scaled_ranks = np.clip(ranks / factor - bound, -bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = [(x, y) for x, y in (X_tr.values.T, scaled_ranks.values.T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_func = interp1d(X_tr, scaled_ranks, kind='linear', fill_value='extrapolate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_func.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = interp_func(X_train['CRIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = scaler.inverse_transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['CRIM'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(z).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = scaler.fit_transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(vv[:, 0]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['CRIM'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(v).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['CRIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(v.shape[0])[~np.isfinite(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = a[a != np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = scaler.inverse_transform(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import FLOAT_DTYPES, check_is_fitted\n",
    "from sklearn.utils import check_array, check_random_state\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.interpolate import interp1d\n",
    "from collections import OrderedDict\n",
    "from numba import jit\n",
    " \n",
    " \n",
    "try:\n",
    "    from joblib import Parallel, delayed\n",
    "except ImportError:\n",
    "    from sklearn.externals.joblib import Parallel, delayed\n",
    " \n",
    " \n",
    "class RankGaussScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nan_to_val=None, extrapolate=False, num_storing=None,\n",
    "                 random_state=None, interp_params=None, n_jobs=None):\n",
    "        nan_to_val = nan_to_val or True\n",
    "        self.nan_to_val = 0.0 if isinstance(nan_to_val, bool) else nan_to_val\n",
    "        self.force_all_finite = False\n",
    "        if isinstance(nan_to_val, bool) and not nan_to_val:\n",
    "            self.force_all_finite = True\n",
    " \n",
    "        self.extrapolate = extrapolate\n",
    "        num_storing = num_storing or np.iinfo(int).max\n",
    "        self.num_storing = 2 if num_storing < 2 else num_storing\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.interp_params = interp_params or dict(kind='linear')\n",
    "        self.n_jobs = n_jobs\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        X = self._check_array(X)\n",
    "        X = self._to_2d_if_1d(X)\n",
    " \n",
    "        self.codebooks_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._make_codebook)(*x) for x in enumerate(X.T))\n",
    " \n",
    "        return self\n",
    " \n",
    "    def transform(self, X):\n",
    "        transformed = self._transform(X, self._transform_column)\n",
    "        if not self.force_all_finite:\n",
    "            transformed[~np.isfinite(transformed)] = self.nan_to_val\n",
    "        return transformed\n",
    " \n",
    "    def inverse_transform(self, X):\n",
    "        return self._transform(X, self._inv_transform_column)\n",
    " \n",
    "    def _transform(self, X, func_transform):\n",
    "        X = self._check_before_transform(X)\n",
    "        return_as_1d = True if len(X.shape) == 1 else False\n",
    "        X = self._to_2d_if_1d(X)\n",
    " \n",
    "        transformed = np.array(\n",
    "            Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(func_transform)(*x, **self.interp_params)\n",
    "                for x in enumerate(X.T))).T\n",
    " \n",
    "        return self._to_1d_if_single(transformed) if return_as_1d \\\n",
    "            else transformed    # preserve input shape\n",
    " \n",
    "    def _check_array(self, X):\n",
    "        # validate input and return X as numpy format\n",
    "        return check_array(X, dtype=FLOAT_DTYPES, ensure_2d=False,\n",
    "                           force_all_finite=self.force_all_finite)\n",
    " \n",
    "    def _check_num_cols(self, X):\n",
    "        # validate input after fit()\n",
    "        num_features = 1 if len(X.shape) == 1 else X.shape[1]\n",
    "        if num_features != len(self.codebooks_):\n",
    "            raise ValueError('bad input shape {0}'.format(X.shape))\n",
    " \n",
    "    def _check_before_transform(self, X):\n",
    "        check_is_fitted(self, 'codebooks_')    # check if 'codebooks_' exists\n",
    "        X = self._check_array(X)       # check input type and structure\n",
    "        self._check_num_cols(X)        # check # of columns\n",
    "        return X\n",
    " \n",
    "    def _make_codebook(self, col_index, x):\n",
    "        codebook = build_rankgauss_trafo(x)\n",
    "        num_codes = len(codebook[0])\n",
    " \n",
    "        if num_codes == 0:\n",
    "            raise ValueError('column %d contains only null values' % col_index)\n",
    "        elif num_codes > self.num_storing:\n",
    "            # first, select minimum and maximum, then choose the rest randomly\n",
    "            chosen = self.random_state.choice(\n",
    "                num_codes - 2, self.num_storing - 2, replace=False)\n",
    "            chosen = np.append(np.array([0, num_codes - 1]), chosen + 1)\n",
    "            return codebook[0][chosen], codebook[1][chosen]\n",
    "        else:\n",
    "            return codebook\n",
    " \n",
    "    def _transform_column(self, index, x, **interp1d_params):\n",
    "        return self._transform_with_interp(\n",
    "            x, *self.codebooks_[index], **interp1d_params)\n",
    " \n",
    "    def _inv_transform_column(self, index, x, **interp1d_params):\n",
    "        return self._transform_with_interp(\n",
    "            x, *reversed(self.codebooks_[index]), **interp1d_params)\n",
    " \n",
    "    def _transform_with_interp(self, x, train_x, train_y, **interp1d_params):\n",
    "        if len(train_x) == 1:\n",
    "            return np.ones(x.shape) * train_y[0]\n",
    "        f = interp1d(train_x, train_y, fill_value='extrapolate',\n",
    "                     **interp1d_params)\n",
    "        return f(x) if self.extrapolate else f(np.clip(x, *minmax(train_x)))\n",
    " \n",
    "    @staticmethod\n",
    "    def _to_2d_if_1d(a):\n",
    "        return a.reshape(-1, 1) if len(a.shape) == 1 else a\n",
    " \n",
    "    @staticmethod\n",
    "    def _to_1d_if_single(a):\n",
    "        return a.ravel() if a.shape[1] == 1 else a\n",
    " \n",
    " \n",
    "# function for simultaneous max() and min() (using numba)\n",
    "# https://stackoverflow.com/a/33919126\n",
    "@jit\n",
    "def minmax(x):\n",
    "    maximum = x[0]\n",
    "    minimum = x[0]\n",
    "    for i in x[1:]:\n",
    "        if i > maximum:\n",
    "            maximum = i\n",
    "        elif i < minimum:\n",
    "            minimum = i\n",
    "    return minimum, maximum\n",
    " \n",
    " \n",
    "# converted from [ref 1]\n",
    "@jit\n",
    "def norm_cdf_inv(p):\n",
    "    sign = 1.0\n",
    "    if p < 0.5:\n",
    "        sign = -1.0\n",
    "    else:\n",
    "        p = 1.0 - p\n",
    "    t = np.sqrt(-2.0 * np.log(p))\n",
    "    return sign * (t - ((0.010328 * t + 0.802853) * t + 2.515517) /\n",
    "                   (((0.001308 * t + 0.189269) * t + 1.432788) * t + 1.0))\n",
    " \n",
    " \n",
    "# converted from [ref 1]\n",
    "@jit\n",
    "def build_rankgauss_trafo(x):\n",
    "    finite_indices = np.isfinite(x)\n",
    "    if np.sum(finite_indices) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    x_finite = x[np.isfinite(x)]\n",
    " \n",
    "    hist = dict()\n",
    "    for val in x_finite:\n",
    "        hist[val] = hist.get(val, 0) + 1\n",
    " \n",
    "    len_hist = len(hist)\n",
    "    list_keys = list(hist.keys())\n",
    " \n",
    "    if len_hist == 1:\n",
    "        return np.array(list_keys), np.array([0.0])\n",
    "    elif len_hist == 2:\n",
    "        return np.array(list_keys), np.array([0.0, 1.0])\n",
    "    else:\n",
    "        hist = OrderedDict(sorted(hist.items()))    # sort by key\n",
    "        n = float(x_finite.shape[0])\n",
    "        cnt = 0.0\n",
    "        mean = 0.0\n",
    "        trafo_keys = list()\n",
    "        trafo_values = list()\n",
    " \n",
    "        for key, val in hist.items():\n",
    "            # (notice) 'cnt / n * 0.998 + 1e-3' is always larger than zero\n",
    "            rank_v = norm_cdf_inv(cnt / n * 0.998 + 1e-3) * 0.7\n",
    "            trafo_keys.append(key)\n",
    "            trafo_values.append(rank_v)\n",
    "            mean += val * rank_v\n",
    "            cnt += val\n",
    " \n",
    "        mean /= n\n",
    "        trafo_values = np.array(trafo_values)\n",
    "        trafo_values -= mean\n",
    " \n",
    "        return np.array(trafo_keys), trafo_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussrankscaler = GaussRankScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "v = gaussrankscaler.fit_transform(X_train['CRIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankgaussscaler = RankGaussScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "x = rankgaussscaler.fit_transform(X_train['CRIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input(X, columns=None, deep=False):\n",
    "    \"\"\"\n",
    "    Unite data into a DataFrame.\n",
    "    Objects that do not contain column names take the names from the argument.\n",
    "    Optionally perform deep copy of the data.\n",
    "    \"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = pd.DataFrame(X, copy=deep)\n",
    "        else:\n",
    "            if columns is not None and np.size(X,1) != len(columns):\n",
    "                raise ValueError('The count of the column names does not correspond to the count of the columns')\n",
    "            if isinstance(X, list):\n",
    "                X = pd.DataFrame(X, columns=columns, copy=deep)  # lists are always copied, but for consistency, we still pass the argument\n",
    "            elif isinstance(X, (np.generic, np.ndarray)):\n",
    "                X = pd.DataFrame(X, columns=columns, copy=deep)\n",
    "            elif isinstance(X, csr_matrix):\n",
    "                X = pd.DataFrame(X.todense(), columns=columns, copy=deep)\n",
    "            else:\n",
    "                raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "\n",
    "            X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    elif deep:\n",
    "        X = X.copy(deep=True)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.all_classes_ = None\n",
    "        self.all_encoders_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit encoder according to X and y.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        # first check the type\n",
    "        X = convert_input(X)\n",
    "        \n",
    "        self._dim = X.shape[1]\n",
    "\n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.cols is None:\n",
    "            self.cols = util.get_obj_cols(X)\n",
    "        else:\n",
    "            self.cols = util.convert_cols_to_list(self.cols)\n",
    "        \n",
    "        if self.handle_missing == 'error':\n",
    "            if X[self.cols].isnull().any().bool():\n",
    "                raise ValueError('Columns to be encoded can not contain null')\n",
    "        \n",
    "        n = len(self.columns)\n",
    "        self.all_classes_ = np.ndarray(shape=(n, ), dtype=object)\n",
    "        self.all_encoders_ = np.ndarray(shape=(n, ), dtype=object)\n",
    "        for i, column in enumerate(self.columns):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X[:, column].astype('str', copy=False))\n",
    "            classes = le.classes_.tolist()\n",
    "            bisect.insort_left(classes, '<unknown>')\n",
    "            le.classes_ = classes\n",
    "            self.all_classes_[i] = (column, np.array(le.classes_, dtype=object))\n",
    "            self.all_encoders_[i] = le\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for i, column in enumerate(self.columns):\n",
    "            X[:, column].astype('str')\n",
    "            X[:, column] = np.where(np.isin(X[:, column], self.all_classes_[i][1]), X[:, column], '<unknown>')\n",
    "            X[:, column] = self.all_encoders_[i].transform(X[:, column])\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X, y=None):\n",
    "        for i, column in enumerate(self.columns):\n",
    "            X[:, column] = self.all_encoders_[i].inverse_transform(X[:, column])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use target encoding to encode two categorical features\n",
    "enc = MultiColumnLabelEncoder()\n",
    "enc.fit(X_train[['CHAS', 'RAD']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the datasets\n",
    "training_numeric_dataset = enc.transform(X_train, y_train)\n",
    "testing_numeric_dataset = enc.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
